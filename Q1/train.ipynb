{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81bed5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import MultivariateNormal\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "\n",
    "num_timesteps = 200 \n",
    "num_trajectories = 10 \n",
    "num_iterations = 100\n",
    "epochs = 100\n",
    "\n",
    "learning_rate = 3e-4\n",
    "eps = 0.2 # clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1892b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.advantages = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def push(self, state, action, reward, advantage, value, log_prob):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.advantages.append(advantage)\n",
    "        self.values.append(value)  \n",
    "        self.log_probs.append(log_prob)\n",
    "\n",
    "    def sample(self):\n",
    "\n",
    "        return (torch.tensor(self.states), \n",
    "                torch.tensor(self.actions), \n",
    "                torch.tensor(self.rewards),\n",
    "                torch.tensor(self.advantages),\n",
    "                torch.tensor(self.values),\n",
    "                torch.tensor(self.log_probs))\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.advantages = []\n",
    "        self.values = []\n",
    "        self.log_probs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e98603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, gamma=0.99, learning_rate = 3e-4):\n",
    "\n",
    "        self.policy_net = Net(3,1)\n",
    "        self.critic_net = Net(3,1)\n",
    "        self.optimizer = torch.optim.Adam([  # Update both models together\n",
    "            {'params': self.policy_net.parameters(), 'lr': learning_rate},\n",
    "            {'params': self.critic_net.parameters(), 'lr': learning_rate}\n",
    "                    ])\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = 1\n",
    "        self.vf_coef = 0.5 # c1\n",
    "        self.entropy_coef = 0.01  # c2\n",
    "        # use fixed std\n",
    "        self.std = torch.diag(torch.full(size=(1,), fill_value=0.5))\n",
    "        self.eps = 0.2\n",
    "    def get_advantages(self,rewards, values, gamma=0.99, lambda_=1):\n",
    "        advantages = torch.zeros_like(torch.as_tensor(rewards))\n",
    "        sum = 0\n",
    "        for t in reversed(range(len(rewards)-1)):\n",
    "            delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
    "            sum = delta + gamma * lambda_ * sum\n",
    "            advantages[t] = sum\n",
    "        return advantages\n",
    "    def generate_trajectory(self):\n",
    "        current_state, info = env.reset()\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "                  \n",
    "        for t in range(num_timesteps):\n",
    "            mean = self.policy_net(torch.as_tensor(current_state))\n",
    "            normal = MultivariateNormal(mean, self.std)\n",
    "\n",
    "            action = normal.sample().detach()\n",
    "            log_prob = normal.log_prob(action).detach()\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            states.append(current_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "        \n",
    "            current_state = next_state\n",
    "        \n",
    "        # calculate values\n",
    "        values = self.critic_net(torch.as_tensor(states)).squeeze()\n",
    "        # calculate advantages\n",
    "        advantages = self.get_advantages(rewards, values.detach(), self.gamma, self.lambda_)\n",
    "        # save the transitions in replay memory\n",
    "        for t in range(len(advantages)):\n",
    "            self.memory.push(states[t], actions[t], rewards[t], advantages[t], values[t], log_probs[t])\n",
    "    def train(self):\n",
    "        \n",
    "        for iter_num in range(num_iterations): # k\n",
    "\n",
    "            # collect a number of trajectories and save the transitions in replay memory\n",
    "            for _ in range(num_trajectories):\n",
    "                self.generate_trajectory()\n",
    "\n",
    "            # sample from replay memory\n",
    "            states, actions, rewards, advantages, values, log_probs = self.memory.sample()\n",
    "            print(\"mean_advantages: \", torch.mean(advantages), end = ' | ')\n",
    "            print(\"mean_values: \", torch.mean(values), end = ' | ')\n",
    "            print(\"mean_reward: \", torch.mean(rewards))\n",
    "\n",
    "            actor_losses = []\n",
    "            critic_losses = []\n",
    "            total_losses = []\n",
    "            #reward_list = []\n",
    "            for e in range(epochs):\n",
    "\n",
    "                # calculate the new log prob\n",
    "                mean = self.policy_net(states)\n",
    "                normal = MultivariateNormal(mean, self.std)\n",
    "                new_log_probs = normal.log_prob(actions.unsqueeze(-1))\n",
    "\n",
    "                r = torch.exp(new_log_probs - log_probs)\n",
    "\n",
    "                clipped_r = torch.clamp(r, 1 - self.eps, 1 + self.eps)\n",
    "\n",
    "                new_values = self.critic_net(states).squeeze()\n",
    "                returns = (advantages + values).detach()\n",
    "\n",
    "                actor_loss = (-torch.min(r * advantages, clipped_r * advantages)).mean()\n",
    "                critic_loss = nn.MSELoss()(new_values.float(), returns.float())\n",
    "\n",
    "                # Calcualte total loss\n",
    "                total_loss = actor_loss + (self.vf_coef * critic_loss) - (self.entropy_coef * normal.entropy().mean())\n",
    "\n",
    "                # update policy and critic network\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward(retain_graph=True)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                actor_losses.append(actor_loss.item())\n",
    "                critic_losses.append(critic_loss.item())\n",
    "                total_losses.append(total_loss.item())\n",
    "                #reward_list.append(sum(rewards))\n",
    "\n",
    "            # clear replay memory\n",
    "            self.memory.clear()\n",
    "\n",
    "\n",
    "\n",
    "            print(\"iteration = \",iter_num, end= ' : ')\n",
    "            print('Actor loss = ', np.mean(actor_losses), end= ' | ')\n",
    "            print('Critic loss = ', np.mean(critic_losses), end= ' | ')\n",
    "            print('Total Loss = ', np.mean(total_losses), end= ' | ')\n",
    "            print(\"\")\n",
    "    def saved(self):\n",
    "        torch.save(self.policy_net.state_dict(), f'policy_net.pt')\n",
    "        torch.save(self.critic_net.state_dict(), f'critic_net.pt')\n",
    "    \n",
    "    def load(self):\n",
    "        self.policy_net.load_state_dict(torch.load(f'policy_net.pt'))\n",
    "        self.critic_net.load_state_dict(torch.load(f'critic_net.pt'))\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        mean = self.policy_net(state)\n",
    "        normal = MultivariateNormal(mean, self.std)\n",
    "        action = normal.sample().detach().numpy()\n",
    "        return action\n",
    "        \n",
    "\n",
    "    def test(self, vedio = False):\n",
    "        self.policy_net.load_state_dict(torch.load(f'policy_net.pt'))\n",
    "        current_state, info = env.reset()\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "        for i in range(200):\n",
    "            mean = self.policy_net(torch.as_tensor(current_state))\n",
    "\n",
    "            normal = MultivariateNormal(mean, self.std)\n",
    "            action = normal.sample().detach().numpy()\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            current_state = next_state\n",
    "\n",
    "        print(total_reward)\n",
    "        imageio.mimsave(\"demo.gif\", frames, fps=30)\n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23374be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO()\n",
    "agent.train()\n",
    "agent.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee6ebd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-239.83775981289864\n"
     ]
    }
   ],
   "source": [
    "agent.test(vedio=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

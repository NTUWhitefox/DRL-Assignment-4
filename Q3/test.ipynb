{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb60b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\DRL\\lib\\site-packages\\gym\\envs\\registration.py:727: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "c:\\Users\\User\\anaconda3\\envs\\DRL\\lib\\site-packages\\pygame\\pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Normal, TransformedDistribution\n",
    "from torch.distributions.transforms import TanhTransform, AffineTransform\n",
    "import imageio\n",
    "from dmc import make_dmc_env\n",
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6955f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import student_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d3742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-1.0, 1.0, (21,), float64)\n",
      "Box(-inf, inf, (67,), float64)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"humanoid-walk\"\n",
    "env = make_dmc_env(env_name, np.random.randint(0, 1000000), flatten=True, use_pixels=False)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6484b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = student_agent.Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24c8e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward:  825.3635457863464\n",
      "Episode reward:  830.2281108619563\n",
      "Episode reward:  864.7034241572363\n",
      "Episode reward:  820.1264907566691\n",
      "Episode reward:  848.3827037535644\n",
      "Episode reward:  848.0579707462218\n",
      "Episode reward:  844.5359794318837\n",
      "Episode reward:  834.4848186334805\n",
      "Episode reward:  855.6602750290283\n",
      "Episode reward:  830.4476745368755\n",
      "Episode reward:  855.4444165121048\n",
      "Episode reward:  1.3785564580073517\n",
      "Episode reward:  851.8424383167143\n",
      "Episode reward:  847.2055459541399\n",
      "Episode reward:  829.1706550961549\n",
      "Episode reward:  839.4753203085328\n",
      "Episode reward:  847.6186292132063\n",
      "Episode reward:  803.2660904106914\n",
      "Episode reward:  818.3023748794903\n",
      "Episode reward:  866.0655403825641\n",
      "Episode reward:  856.7785218111956\n",
      "Episode reward:  846.6694797079648\n",
      "Episode reward:  783.3754864395643\n",
      "Episode reward:  772.2631082093385\n",
      "Episode reward:  852.8670711345355\n",
      "Episode reward:  823.9726061999874\n",
      "Episode reward:  841.464656197829\n",
      "Episode reward:  848.4499198841065\n",
      "Episode reward:  688.2042615259585\n",
      "Episode reward:  834.5519486724498\n",
      "Episode reward:  736.3821663740243\n",
      "Episode reward:  844.3473382287668\n",
      "Episode reward:  796.832385311069\n",
      "Episode reward:  853.3149966099202\n",
      "Episode reward:  855.1496748502641\n",
      "Episode reward:  809.0824218873664\n",
      "Episode reward:  807.8187375680277\n",
      "Episode reward:  857.0105272146235\n",
      "Episode reward:  836.0998493450834\n",
      "Episode reward:  823.8999113556168\n",
      "Episode reward:  858.3650239014978\n",
      "Episode reward:  835.6968246178245\n",
      "Episode reward:  840.8291151953798\n",
      "Episode reward:  759.631436293144\n",
      "Episode reward:  815.0150521220226\n",
      "Episode reward:  805.1759190583383\n",
      "Episode reward:  746.9259998525159\n",
      "Episode reward:  847.2314658463212\n",
      "Episode reward:  841.5624015301247\n",
      "Episode reward:  737.0214408897156\n",
      "Episode reward:  821.2189174714159\n",
      "Episode reward:  859.6799369658318\n",
      "Episode reward:  762.7347037481471\n",
      "Episode reward:  849.7453502930291\n",
      "Episode reward:  821.8695305306165\n",
      "Episode reward:  861.6933279554673\n",
      "Episode reward:  851.3862728436519\n",
      "Episode reward:  820.5400399395625\n",
      "Episode reward:  844.6043230197021\n",
      "Episode reward:  841.2974218849207\n",
      "Episode reward:  850.0651201209155\n",
      "Episode reward:  857.7057183925448\n",
      "Episode reward:  848.6352100082884\n",
      "Episode reward:  844.8642795250936\n",
      "Episode reward:  845.9729924503608\n",
      "Episode reward:  796.1720460897421\n",
      "Episode reward:  788.1582133264235\n",
      "Episode reward:  833.5904973915356\n",
      "Episode reward:  858.2881701066309\n",
      "Episode reward:  820.5807749817334\n",
      "Episode reward:  831.9946472717628\n",
      "Episode reward:  852.3356135383084\n",
      "Episode reward:  857.9142674028724\n",
      "Episode reward:  837.1341487920187\n",
      "Episode reward:  754.2126957967289\n",
      "Episode reward:  732.8070430179007\n",
      "Episode reward:  852.3925690729859\n",
      "Episode reward:  780.8404175187927\n",
      "Episode reward:  839.0430414786639\n",
      "Episode reward:  849.0169835004999\n",
      "Episode reward:  838.6086534212068\n",
      "Episode reward:  872.4623595317466\n",
      "Episode reward:  773.8218305389037\n",
      "Episode reward:  855.9410878871709\n",
      "Episode reward:  817.3546087625199\n",
      "Episode reward:  796.1303827587838\n",
      "Episode reward:  830.2272649745353\n",
      "Episode reward:  731.0107816468176\n",
      "Episode reward:  859.2073404540071\n",
      "Episode reward:  814.8701165202316\n",
      "Episode reward:  838.827422639329\n",
      "Episode reward:  853.5024091383355\n",
      "Episode reward:  831.9797292604826\n",
      "Episode reward:  842.9156044085039\n",
      "Episode reward:  857.0692350306851\n",
      "Episode reward:  856.9725522945006\n",
      "Episode reward:  838.7602773428183\n",
      "Episode reward:  837.7916452844862\n",
      "Episode reward:  834.3748409070006\n",
      "Episode reward:  858.9800443256355\n",
      "729.8116642080895\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for _ in range(100):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "    print(\"Episode reward: \", episode_reward)\n",
    "    rewards.append(episode_reward)\n",
    "print(np.mean(rewards) - np.std(rewards))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

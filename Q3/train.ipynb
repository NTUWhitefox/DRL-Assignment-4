{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f0333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\DRL\\lib\\site-packages\\gym\\envs\\registration.py:727: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "c:\\Users\\User\\anaconda3\\envs\\DRL\\lib\\site-packages\\pygame\\pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Normal, TransformedDistribution\n",
    "from torch.distributions.transforms import TanhTransform, AffineTransform\n",
    "import imageio\n",
    "from dmc import make_dmc_env\n",
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ceac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdb71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-1.0, 1.0, (21,), float64)\n",
      "Box(-inf, inf, (67,), float64)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"humanoid-walk\"\n",
    "env = make_dmc_env(env_name, np.random.randint(0, 1000000), flatten=True, use_pixels=False)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f74db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "frames = []\n",
    "for _ in range(100):\n",
    "    _,_,_,_,_ = env.step(env.action_space.sample())\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "imageio.mimsave(\"humanoid.gif\", frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9dab967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 21\n"
     ]
    }
   ],
   "source": [
    "print(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94481df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = ('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff4cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure stability of neural networks learning\n",
    "LOG_STD_MIN = 2\n",
    "LOG_STD_MAX = -20\n",
    "EPS = 1e-6\n",
    "# absolute bounds for weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9adb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replay Buffer ---\n",
    "class ReplayBufferSAC:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Ensure inputs are numpy arrays for consistent stacking later\n",
    "        state = np.asarray(state)\n",
    "        action = np.asarray(action)\n",
    "        reward = np.asarray([reward]) # Store reward as an array\n",
    "        next_state = np.asarray(next_state)\n",
    "        done = np.asarray([done])     # Store done as an array\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done.astype(np.float32) # Ensure done is float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "271a2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module): # V(s) - state value (kept as per user, but not used in this SAC variant)\n",
    "    def __init__(self, input_size, output_size=1, hidden_size=128):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "# Q(s,a) - action value\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128): # input_size = state_dim + action_dim, output_size = 1\n",
    "        super(QNet, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1) # Output a single Q-value\n",
    "        )\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        return self.seq(x)\n",
    "\n",
    "# Policy Network pi(a|s) for SAC\n",
    "class PolicyNetSAC(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128, action_bound_val=1.0):\n",
    "        super(PolicyNetSAC, self).__init__()\n",
    "        self.action_dim = action_dim\n",
    "        # Assuming action_bound_val means actions are in [-action_bound_val, action_bound_val]\n",
    "        # For actions in [-1, 1], action_bound_val = 1.0\n",
    "        self.action_scale = torch.tensor(action_bound_val)\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_layer = nn.Linear(hidden_size, action_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.seq(state)\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        sample = dist.rsample()\n",
    "        action = torch.tanh(sample)\n",
    "        log_prob = dist.log_prob(sample) - torch.log(1 - action.pow(2) + EPS)\n",
    "        log_prob = log_prob.sum(axis=-1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).unsqueeze(0).to(device, dtype=torch.float)\n",
    "        true_action, _ = self.sample(state)\n",
    "        return true_action.detach().cpu().numpy()\n",
    "    \n",
    "# --- SAC Agent ---\n",
    "class SAC_agent:\n",
    "    def __init__(self, state_dim, action_dim, device,\n",
    "                 hidden_size=128, # User requested hidden_size for networks\n",
    "                 lr_actor=3e-4, lr_critic=3e-4, lr_alpha=3e-4,\n",
    "                 gamma=0.99, tau=0.005,\n",
    "                 alpha=0.2, # Initial alpha, can be learned\n",
    "                 target_update_interval=1,\n",
    "                 replay_capacity=1000000,\n",
    "                 action_bound_val=1.0): # Corresponds to action values in [-action_bound_val, action_bound_val]\n",
    "\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha # Will be learnable if log_alpha is defined\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor = PolicyNetSAC(state_dim, action_dim, hidden_size, action_bound_val=action_bound_val).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "\n",
    "        # Critic Networks (Twinned Q-networks)\n",
    "        self.critic1 = QNet(state_dim, action_dim, hidden_size).to(self.device)\n",
    "        self.critic1_target = QNet(state_dim, action_dim, hidden_size).to(self.device)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "        #self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=lr_critic)\n",
    "\n",
    "        self.critic2 = QNet(state_dim, action_dim, hidden_size).to(self.device)\n",
    "        self.critic2_target = QNet(state_dim, action_dim, hidden_size).to(self.device)\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "        #self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=lr_critic)\n",
    "        self.critic_optimizer = optim.Adam(list(self.critic1.parameters()) + list(self.critic2.parameters()), lr = lr_critic)\n",
    "        \n",
    "        # Automatic Entropy Tuning (for alpha)\n",
    "        self.target_entropy = -torch.prod(torch.Tensor(self.action_dim).to(device)).item()\n",
    "\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha = self.log_alpha.exp().item()\n",
    "        #self.alpha_optimizer = optim.Adam([self.alpha], lr=lr_alpha)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr_alpha)\n",
    "\n",
    "        self.replay_buffer = ReplayBufferSAC(replay_capacity)\n",
    "        self.updates = 0 # Counter for target updates\n",
    "\n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \n",
    "        if evaluate: # Deterministic action\n",
    "            action  = self.actor.get_action(state)\n",
    "            return action\n",
    "        else: # Stochastic action\n",
    "            state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "            action, _ = self.actor.sample(state_tensor) # No reparam needed for just acting\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update_parameters(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return None, None, None # Not enough samples to train\n",
    "\n",
    "        state_np, action_np, reward_np, next_state_np, done_np = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        state = torch.FloatTensor(state_np).to(self.device)\n",
    "        action = torch.FloatTensor(action_np).to(self.device)\n",
    "        reward = torch.FloatTensor(reward_np).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state_np).to(self.device)\n",
    "        done = torch.FloatTensor(done_np).to(self.device)\n",
    "\n",
    "        # --- Update Critic Networks ---\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample(next_state)\n",
    "            q1_next_target = self.critic1_target(next_state, next_actions)\n",
    "            q2_next_target = self.critic2_target(next_state, next_actions)\n",
    "            min_q_next_target = torch.min(q1_next_target, q2_next_target) - self.alpha * next_log_probs\n",
    "            next_q_value = reward + (1 - done) * self.gamma * min_q_next_target\n",
    "        \n",
    "        q1 = self.critic1(state, action)\n",
    "        q2 = self.critic2(state, action)\n",
    "        \n",
    "        critic1_loss = F.mse_loss(q1, next_q_value)\n",
    "        critic2_loss = F.mse_loss(q2, next_q_value)\n",
    "        critic_total_loss = critic1_loss+critic2_loss\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_total_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        #self.critic1_optimizer.zero_grad()\n",
    "        #critic1_loss.backward()\n",
    "        #self.critic1_optimizer.step()\n",
    "\n",
    "        #self.critic2_optimizer.zero_grad()\n",
    "        #critic2_loss.backward()\n",
    "        #self.critic2_optimizer.step()\n",
    "\n",
    "        # --- Update Actor Network ---\n",
    "        # Freeze Q-networks to prevent gradient flow from actor loss\n",
    "        #for p in self.critic1.parameters(): p.requires_grad = False\n",
    "        #for p in self.critic2.parameters(): p.requires_grad = False\n",
    "\n",
    "        pi_actions, pi_log_probs = self.actor.sample(state) # Uses reparameterization\n",
    "        q1_pi = self.critic1(state, pi_actions)\n",
    "        q2_pi = self.critic2(state, pi_actions)\n",
    "        min_q_pi = torch.min(q1_pi, q2_pi)\n",
    "        \n",
    "        actor_loss = ((self.alpha * pi_log_probs) - min_q_pi).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Unfreeze Q-networks\n",
    "        #for p in self.critic1.parameters(): p.requires_grad = True\n",
    "        #for p in self.critic2.parameters(): p.requires_grad = True\n",
    "\n",
    "        # --- Update Alpha (Temperature) ---\n",
    "        alpha_loss = -(self.log_alpha.exp() * (pi_log_probs + self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        self.alpha = self.log_alpha.exp().item()\n",
    "\n",
    "        # --- Soft Update Target Networks ---\n",
    "        self.updates +=1\n",
    "        if self.updates % self.target_update_interval == 0:\n",
    "            self._soft_update(self.critic1_target, self.critic1)\n",
    "            self._soft_update(self.critic2_target, self.critic2)\n",
    "            \n",
    "        return critic1_loss.item(), actor_loss.item(), self.alpha\n",
    "\n",
    "\n",
    "    def _soft_update(self, target_net, source_net):\n",
    "        for target_param, source_param in zip(target_net.parameters(), source_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * source_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def save_checkpoint(self, path, filename_prefix=\"sac_checkpoint\"):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        checkpoint = {\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic1_state_dict': self.critic1.state_dict(),\n",
    "            'critic2_state_dict': self.critic2.state_dict(),\n",
    "            'critic1_target_state_dict': self.critic1_target.state_dict(),\n",
    "            'critic2_target_state_dict': self.critic2_target.state_dict(),\n",
    "            'alpha_state_dict': self.alpha, # Save log_alpha tensor directly\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "            'alpha_optimizer_state_dict': self.alpha_optimizer.state_dict(),\n",
    "            'replay_buffer': self.replay_buffer.buffer # Save deque content\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(path, f\"{filename_prefix}.pth\"))\n",
    "        print(f\"SAC checkpoint saved to {os.path.join(path, f'{filename_prefix}.pth')}\")\n",
    "\n",
    "    def load_checkpoint(self, path, filename_prefix=\"sac_checkpoint\"):\n",
    "        checkpoint_path = os.path.join(path, f\"{filename_prefix}.pth\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            print(checkpoint_path)\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)\n",
    "            self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "            self.critic1.load_state_dict(checkpoint['critic1_state_dict'])\n",
    "            self.critic2.load_state_dict(checkpoint['critic2_state_dict'])\n",
    "            self.critic1_target.load_state_dict(checkpoint['critic1_target_state_dict'])\n",
    "            self.critic2_target.load_state_dict(checkpoint['critic2_target_state_dict'])\n",
    "            \n",
    "            # Load log_alpha\n",
    "            # If saved as a tensor:\n",
    "            self.alpha = checkpoint['alpha_state_dict']\n",
    "\n",
    "            self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "            self.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer_state_dict'])\n",
    "            \n",
    "            self.replay_buffer.buffer = checkpoint.get('replay_buffer', deque(maxlen=self.replay_buffer.buffer.maxlen))\n",
    "\n",
    "            # Update current alpha value from loaded log_alpha\n",
    "            #self.alpha = self.alpha.exp().detach().item()\n",
    "            print(f\"SAC checkpoint loaded from {checkpoint_path}\")\n",
    "            # Ensure target networks are identical to loaded main networks initially after loading\n",
    "            self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "            self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "            return 0\n",
    "        else:\n",
    "            print(f\"No SAC checkpoint found at {checkpoint_path}, starting from scratch.\")\n",
    "            return -1\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes, max_episode_len, seed_offset=100000):\n",
    "    total_reward = 0\n",
    "    for i in range(num_episodes):\n",
    "        # Use a different seed for each evaluation episode for more robust evaluation\n",
    "        # Pass the seed to env.reset if your wrapper supports it.\n",
    "        # If your make_dmc_env creates a new env instance, you can pass seed there.\n",
    "        # For now, assuming env.reset() can take a seed.\n",
    "        obs, _ = env.reset(seed=seed_offset + i)\n",
    "        episode_reward = 0\n",
    "        for _ in range(max_episode_len):\n",
    "            action = agent.select_action(obs, evaluate=True)\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            obs = next_obs\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        total_reward += episode_reward\n",
    "    return total_reward / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c843869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, num_episodes, max_episode_len, seed_offset=100000):\n",
    "    total_reward = 0\n",
    "    for i in range(num_episodes):\n",
    "        # Use a different seed for each evaluation episode for more robust evaluation\n",
    "        # Pass the seed to env.reset if your wrapper supports it.\n",
    "        # If your make_dmc_env creates a new env instance, you can pass seed there.\n",
    "        # For now, assuming env.reset() can take a seed.\n",
    "        obs, _ = env.reset(seed=seed_offset + i)\n",
    "        episode_reward = 0\n",
    "        for _ in range(max_episode_len):\n",
    "            action = agent.select_action(obs, evaluate=True)\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            obs = next_obs\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        total_reward += episode_reward\n",
    "    return total_reward / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49128288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "State dim: 67, Action dim: 21\n",
      "Action space low: -1.0, high: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- Hyperparameters ---\n",
    "    ENV_NAME = \"humanoid-walk\"\n",
    "    SEED = np.random.randint(0, 1000000) # Initial seed for the training environment\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # SAC Agent Hyperparameters\n",
    "    HIDDEN_SIZE = 256 # Increased from 128, better for humanoid\n",
    "    LR_ACTOR = 3e-4\n",
    "    LR_CRITIC = 3e-4\n",
    "    LR_ALPHA = 3e-4 # Learning rate for temperature\n",
    "    GAMMA = 0.99    # Discount factor\n",
    "    TAU = 0.005     # Target network soft update rate\n",
    "    ALPHA_INIT = 0.2 # Initial temperature, will be tuned\n",
    "    REPLAY_CAPACITY = int(1e6)\n",
    "    ACTION_BOUND_VAL = 1.0 # From env.action_space: Box(-1.0, 1.0, ...)\n",
    "\n",
    "    # Training Loop Hyperparameters\n",
    "    TOTAL_TIMESTEPS = int(1e7)       # Total timesteps for training\n",
    "    START_TIMESTEPS = int(1e4)       # Timesteps for random actions before training starts\n",
    "    BATCH_SIZE = 256                 # Batch size for SAC updates\n",
    "    UPDATES_PER_STEP = 1             # Number of SAC updates per environment step\n",
    "    MAX_EPISODE_LEN = 1000           # Max length of each episode (typical for DMC humanoid)\n",
    "    \n",
    "    EVAL_FREQ = int(2e4)             # Evaluate agent every N timesteps\n",
    "    EVAL_EPISODES = 10               # Number of episodes for evaluation\n",
    "    SAVE_FREQ = int(1e5)             # Save model every N timesteps\n",
    "    MODEL_SAVE_PATH = \"./sac_models\" # Path to save models\n",
    "\n",
    "    # --- Initialization ---\n",
    "    # Note: For DMC, ensure your `make_dmc_env` correctly sets up the environment.\n",
    "    # The `flatten=True` and `use_pixels=False` are specific to how you wrap it.\n",
    "    train_env = make_dmc_env(ENV_NAME, SEED, flatten=True, use_pixels=False)\n",
    "    # Create a separate environment for evaluation with a different seed if possible\n",
    "    eval_env = make_dmc_env(ENV_NAME, SEED + 12345, flatten=True, use_pixels=False) \n",
    "\n",
    "    state_dim = train_env.observation_space.shape[0]\n",
    "    action_dim = train_env.action_space.shape[0]\n",
    "    print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "    print(f\"Action space low: {train_env.action_space.low[0]}, high: {train_env.action_space.high[0]}\")\n",
    "\n",
    "\n",
    "    agent = SAC_agent(state_dim, action_dim, DEVICE,\n",
    "                      hidden_size=HIDDEN_SIZE,\n",
    "                      lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, lr_alpha=LR_ALPHA,\n",
    "                      gamma=GAMMA, tau=TAU, alpha=ALPHA_INIT,\n",
    "                      replay_capacity=REPLAY_CAPACITY,\n",
    "                      action_bound_val=ACTION_BOUND_VAL)\n",
    "\n",
    "    # Optional: Load checkpoint if resuming\n",
    "    # agent.load_checkpoint(MODEL_SAVE_PATH, \"sac_checkpoint_some_timestep\")\n",
    "\n",
    "    obs, _ = train_env.reset(seed=SEED)\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "    \n",
    "    # For logging losses\n",
    "    recent_critic_loss = 0\n",
    "    recent_actor_loss = 0\n",
    "    recent_alpha = agent.alpha\n",
    "\n",
    "    print(f\"Starting training for {TOTAL_TIMESTEPS} timesteps...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for t in range(TOTAL_TIMESTEPS):\n",
    "        episode_timesteps += 1\n",
    "\n",
    "        if t < START_TIMESTEPS:\n",
    "            action = train_env.action_space.sample() # Random action\n",
    "        else:\n",
    "            action = agent.select_action(obs, evaluate=False)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = train_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.store_transition(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "\n",
    "        if t >= START_TIMESTEPS:\n",
    "            for _ in range(UPDATES_PER_STEP):\n",
    "                c_loss, a_loss, current_alpha_val = agent.update_parameters(BATCH_SIZE)\n",
    "                if c_loss is not None: # If an update happened\n",
    "                    recent_critic_loss = c_loss\n",
    "                    recent_actor_loss = a_loss\n",
    "                    recent_alpha = current_alpha_val\n",
    "        \n",
    "        if done or episode_timesteps >= MAX_EPISODE_LEN: # MAX_EPISODE_LEN handles non-terminating tasks\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Total T: {t+1}/{TOTAL_TIMESTEPS} | Episode Num: {episode_num+1} | Episode T: {episode_timesteps} | Reward: {episode_reward:.2f} | Alpha: {recent_alpha:.4f} | C_Loss: {recent_critic_loss:.8f} | A_Loss: {recent_actor_loss:.8f} | Time: {elapsed_time/60:.1f}m\")\n",
    "            \n",
    "            obs, _ = train_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "\n",
    "        if (t + 1) % EVAL_FREQ == 0 and t >= START_TIMESTEPS:\n",
    "            avg_eval_reward = evaluate_agent(eval_env, agent, EVAL_EPISODES, MAX_EPISODE_LEN)\n",
    "            print(\"--------------------------------------------------------\")\n",
    "            print(f\"Evaluation at T: {t+1} | Avg Reward over {EVAL_EPISODES} episodes: {avg_eval_reward:.2f}\")\n",
    "            print(\"--------------------------------------------------------\")\n",
    "\n",
    "        if (t + 1) % SAVE_FREQ == 0 and t >= START_TIMESTEPS :\n",
    "            agent.save_checkpoint(MODEL_SAVE_PATH, f\"sac_humanoid_t{t+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee752dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sac_models\\sac_humanoid_t5600000.pth\n",
      "SAC checkpoint loaded from ./sac_models\\sac_humanoid_t5600000.pth\n",
      "Average evaluation reward at step 5600000: 804.01\n",
      "No SAC checkpoint found at ./sac_models\\sac_humanoid_t5700000.pth, starting from scratch.\n",
      "./sac_models\\sac_humanoid_t5800000.pth\n",
      "SAC checkpoint loaded from ./sac_models\\sac_humanoid_t5800000.pth\n",
      "Average evaluation reward at step 5800000: 727.81\n",
      "./sac_models\\sac_humanoid_t5900000.pth\n",
      "SAC checkpoint loaded from ./sac_models\\sac_humanoid_t5900000.pth\n",
      "Average evaluation reward at step 5900000: 482.80\n",
      "Best reward: 804.01 at step: 5600000\n"
     ]
    }
   ],
   "source": [
    "#load all file in sac_models and find the best one with average total reward over 100 episodes\n",
    "#agent.load_checkpoint(MODEL_SAVE_PATH, \"sac_checkpoint_some_timestep\")\n",
    "\n",
    "step_range = (5600000, 5900001)\n",
    "test_agent = SAC_agent(state_dim, action_dim, DEVICE,\n",
    "                      hidden_size=HIDDEN_SIZE,\n",
    "                      lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, lr_alpha=LR_ALPHA,\n",
    "                      gamma=GAMMA, tau=TAU, alpha=ALPHA_INIT,\n",
    "                      replay_capacity=REPLAY_CAPACITY,\n",
    "                      action_bound_val=ACTION_BOUND_VAL)\n",
    "best_step = 0\n",
    "best_reward = -np.inf\n",
    "for steps in range(step_range[0], step_range[1] + 1, 100000):\n",
    "    if test_agent.load_checkpoint(MODEL_SAVE_PATH, f\"sac_humanoid_t{steps}\") == -1:\n",
    "        continue\n",
    "    avg_eval_reward = evaluate_agent(eval_env, test_agent, 100, 2000)\n",
    "    print(f\"Average evaluation reward at step {steps}: {avg_eval_reward:.2f}\")\n",
    "    if avg_eval_reward > best_reward:\n",
    "        best_reward = avg_eval_reward\n",
    "        best_step = steps\n",
    "print(f\"Best reward: {best_reward:.2f} at step: {best_step}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67200c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sac_humanoid_t8700000.pth\n",
      "SAC checkpoint loaded from ./sac_humanoid_t8700000.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dump ou polycy only\n",
    "HIDDEN_SIZE = 256 # Increased from 128, better for humanoid\n",
    "LR_ACTOR = 3e-4\n",
    "LR_CRITIC = 3e-4\n",
    "LR_ALPHA = 3e-4 # Learning rate for temperature\n",
    "GAMMA = 0.99    # Discount factor\n",
    "TAU = 0.005     # Target network soft update rate\n",
    "ALPHA_INIT = 0.2 # Initial temperature, will be tuned\n",
    "REPLAY_CAPACITY = int(1e6)\n",
    "ACTION_BOUND_VAL = 1.0 # From env.action_space: Box(-1.0, 1.0, ...)\n",
    "\n",
    "    # Training Loop Hyperparameters\n",
    "TOTAL_TIMESTEPS = int(1e7)       # Total timesteps for training\n",
    "START_TIMESTEPS = int(1e4)       # Timesteps for random actions before training starts\n",
    "BATCH_SIZE = 256                 # Batch size for SAC updates\n",
    "UPDATES_PER_STEP = 1             # Number of SAC updates per environment step\n",
    "MAX_EPISODE_LEN = 1000           # Max length of each episode (typical for DMC humanoid)\n",
    "    \n",
    "EVAL_FREQ = int(2e4)             # Evaluate agent every N timesteps\n",
    "EVAL_EPISODES = 10               # Number of episodes for evaluation\n",
    "SAVE_FREQ = int(1e5)             # Save model every N timesteps\n",
    "MODEL_SAVE_PATH = \"./\" # Path to save models\n",
    "test_agent = SAC_agent(state_dim, action_dim, DEVICE,\n",
    "                      hidden_size=HIDDEN_SIZE,\n",
    "                      lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, lr_alpha=LR_ALPHA,\n",
    "                      gamma=GAMMA, tau=TAU, alpha=ALPHA_INIT,\n",
    "                      replay_capacity=REPLAY_CAPACITY,\n",
    "                      action_bound_val=ACTION_BOUND_VAL)\n",
    "\n",
    "test_agent.load_checkpoint(MODEL_SAVE_PATH, f\"sac_humanoid_t{8700000}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5675c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_agent.actor.state_dict(), \"policy.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
